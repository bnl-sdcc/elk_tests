#
# LogStash pipeline as front end to the ElasticSearch cluster
# This LogStash instance collects data from all other instances
# at the batch nodes, aggregates the information, 
# manipulates it a little bit, and sends the result to ElasticSearch
#
# This pipeline was written for LogStash 6.6.2
#
# This files goes under /etc/logstash/conf.d/
#
# Parameters for customization:
#     - port, in the lumberjack input plugin
#     - hosts, in the elasticsearch output plugin
#
# author: Jose Caballero <jcaballero@bnl.gov>
#

input {
    lumberjack {
        codec => json
        port => 5555
        ssl_certificate => "/etc/logstash/certs/lumberjack.cert"
        ssl_key => "/etc/logstash/certs/lumberjack.key"
    }
}


filter {
    mutate {
        remove_field => [ "@version", "tags" ]
    }

    #mutate {
    #    copy => { "starttime" => "tmpindex" }
    #}
    #mutate {
    #    split => ["tmpindex", " "]
    #    add_field => { "index" => "%{[tmpindex][0]}" }
    #}
    #mutate {
    #    gsub => [ "index", "/", "." ]
    #}
    #mutate {
    #    replace => { "index" => "htcondorjobs-%{index}" }
    #}
    #mutate {
    #    remove_field => [ "tmpindex" ]
    #}

    ruby {
        code => '
           # convert starttime to index
           # starttime has formats
           #    - 01/08/19 08:42:54
           #    - 01/08/19 08:42:54.345
           # index has format "htcondorjobs-<YYYY.MM.DD>"
           date = Date.strptime(event.get("starttime"), "%m/%d/%y %H:%M:%S").strftime("%Y.%m.%d")
           event.set("index", "htcondorjobs-" + date)

           # document_id has format <globaljobid>_<trial>
           event.set("document_id", event.get("globaljobid") + "_" + event.get("trial").to_s)
        '
    }


    # to assign the content of starttime to @timestamp
    # in order to "Discover" data in ElasticSearch based on the date
    # the job started, not when the data was dumped into ElasticSearch
    # This is to avoid the many documents being recorded upon start 
    # (corresponding to several days)
    # to collapse Kibana
    date {
        match => [ "starttime" , "MM/dd/yy HH:mm:ss.SSS", "MM/dd/yy HH:mm:ss", "MM/dd/yy HH:mm:ss.1000"]
    }


}


#output {
#   stdout { codec => rubydebug }
#}

output {
    elasticsearch {
        hosts => [ "************************:9210" ]
        index => "%{index}"
        document_id => "%{document_id}"
        action => "update"
        doc_as_upsert => true
    } 
}

