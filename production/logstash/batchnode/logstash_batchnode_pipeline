#
# LogStash pipeline for the HTCondor batch nodes
# This LogStash instance collects data from FileBeat,
# manipulates it, and sends to the LogStash in front of ElasticSearch
#
# This pipeline was written for LogStash 6.6.2
#
# This files goes under /etc/logstash/conf.d/
#
# Parameters for customization:
#     - port, in the beats input plugin
#     - number of seconds to reject too old data
#     - hosts, in the lumberjack output plugin
#
# author: Jose Caballero <jcaballero@bnl.gov>
#




input {
    beats {
        port => "5044"
    }
}

filter {

    # =========================================================================
    #                           StarterLog
    # -------------------------------------------------------------------------
    # here we process the events from the StarterLog.slot1_* files
    # FileBeat marks these Events with extra field "log_type" = "starterlog"
    # =========================================================================


    if [fields][log_type] == "starterlog" {

        grok {
            pattern_definitions => { "START_TIMESTAMP" => "%{DATE_US} %{TIME}" }
            match => { "message" => [
                '^%{START_TIMESTAMP:starttime} \*\* PID = ',
                '^%{START_TIMESTAMP} Submitting machine is "%{HOSTNAME:submithost}"',
                '^GlobalJobId = "%{GREEDYDATA:globaljobid}"',
                '^%{START_TIMESTAMP} Starting a VANILLA universe job with ID: %{NUMBER:jobid}',
                '^%{START_TIMESTAMP} Running job as user %{WORD:user}',
                '^%{START_TIMESTAMP:endtime} Notifying exit status=%{NUMBER:status} reason=%{NUMBER:reason}',
                '^%{START_TIMESTAMP:eventtimestamp}'
            ] }
            # we do not break on match because 
            # we need to keep matching even when the rule 
            #    '^%{START_TIMESTAMP:eventtimestamp}'
            # is true itself 
            break_on_match => false
        }

        date {
            # convert the content of field "eventtimestamp" into a Timestamp object
            # eventtimestamp looks like "03/29/19 15:08:08.841"
            match => ["eventtimestamp", "MM/dd/yy HH:mm:ss.SSS"]
            target => "eventtimestamp"
        }

        ruby { 

            init => '
                # Before anything else, we create the dictionaries (empty)
                # to host all relevant variables
                $starttime_h = Hash.new
                $starttimebackup_h = Hash.new
                $globaljobid_h = Hash.new
                $jobid_h = Hash.new
                $submithost_h = Hash.new
                $user_h = Hash.new
                $hasuser_h = Hash.new
                $hasstatus_h = Hash.new

                # we need to discard too old events, 
                # to avoid dumping too much data into into ElasticSearch
                $recentenough_h = Hash.new

                # explanation for return codes, as it is written in the HTCondor manual
                $codes = Hash.new
                $codes[4] = "JOB_EXCEPTION: The job exited with an exception"
                $codes[44] = "DPRINTF_ERROR: There is a fatal error with dprintf()"
                $codes[100] = "JOB_EXITED: The job exited (not killed)"
                $codes[101] = "JOB_CKPTED: The job was checkpointed"
                $codes[102] = "JOB_KILLED: The job was killed"
                $codes[103] = "JOB_COREDUMPED: The job was killed and a core file produced"
                $codes[105] = "JOB_NO_MEM: Not enough memory to start the shadow"
                $codes[106] = "JOB_SHADOW_USAGE: incorrect arguments to condor_shadow"
                $codes[107] = "JOB_NOT_CKPTED: The job was kicked off without a checkpoint. JOB_SHOULD_REQUEUE: The effect of this exit code is that we want the job to be put back in the job queue and run again."
                $codes[108] = "JOB_NOT_STARTED: Cannot connect to startd or request refused"
                $codes[109] = "JOB_BAD_STATUS: Job status != RUNNING on startup"
                $codes[110] = "JOB_EXEC_FAILED: failed for some reason other than ENOMEM"
                $codes[111] = "JOB_NO_CKPT_FILE: There is no checkpoint file (lost)"
                $codes[112] = "JOB_SHOULD_HOLD: job should be put on hold"
                $codes[113] = "JOB_SHOULD_REMOVE: The job should be removed"
            '

            code => '

                # log filename
                # we will use it as key in several dictionaries
                # to identify which line comes from each file and prevent mixing them
                $source = event.get("source")

                # check if we can start processing events
                # or they are still too old
                # we check if the starting line for a new job 
                # is younger than 10 days 
                # we focus on that line in particular to avoid
                # starting the processing in the middle of a running job
                if (! $recentenough_h[$source] ) && event.get("starttime")
                    if (Time.now.to_i - event.get("eventtimestamp").to_i) < (3600*24*10)
                        $recentenough_h[$source] = true
                    end
                end
                if ! $recentenough_h[$source]
                    event.cancel
                elsif

                    # data is recent enough for processing...
                    
                    if event.get("starttime")

                        $starttime_h[$source] = event.get("starttime")

                        if defined?($hasuser_h[$source]) && $hasstatus_h[$source] == false
                            # if we are here is because all of this is true:
                            #   -- we are in the line for a new job, so we are done processing previous one
                            #   -- there was an username associated to the previous job
                            #   -- there was no final job status associated to the previous job
                            # all of that means that the previous job crashed
                            event.set("starttime", $starttimebackup_h[$source])
                            event.set("jobid", $jobid_h[$source])
                            event.set("globaljobid", $globaljobid_h[$source])
                            event.set("submithost", $submithost_h[$source])
                            event.set("user", $user_h[$source])
                            event.set("status", "CRASHED")
                            # we sent the event data to the output
                        else
                            event.cancel
                        end
                        
                        $starttimebackup_h[$source] = $starttime_h[$source]


                    elsif event.get("submithost")
                        # we record the value of submithost. It will be used later on, together with other fields
                        $submithost_h[$source] = event.get("submithost")
                        event.cancel

                    elsif event.get("globaljobid")
                        # we record the value of globaljobid. It will be used later on, together with other fields
                        $globaljobid_h[$source] = event.get("globaljobid")
                        event.cancel

                    elsif event.get("jobid")
                        # we record the value of jobid. It will be used later on, together with other fields
                        $jobid_h[$source] = event.get("jobid")
                        event.cancel

                    elsif event.get("user")
                        $user_h[$source] = event.get("user")
                        event.set("starttime", $starttime_h[$source])
                        event.set("globaljobid", $globaljobid_h[$source])
                        event.set("jobid", $jobid_h[$source])
                        event.set("submithost", $submithost_h[$source])
                        event.set("user", $user_h[$source])
                        # we sent the event data to the output

                        $hasuser_h[$source] = true
                        $hasstatus_h[$source] = false


                    elsif event.get("status")
                        event.set("starttime", $starttime_h[$source])
                        event.set("globaljobid", $globaljobid_h[$source])
                        event.set("jobid", $jobid_h[$source])
                        event.set("submithost", $submithost_h[$source])
                        event.set("user", $user_h[$source])

                        # add explanatory message for return codes
                        msg = $codes[event.get("reason").to_i]
                        event.set("explanation", msg)

                        # we sent the event data to the output

                        $hasstatus_h[$source] = true

                    else
                        event.cancel
                    end

                end            
            '
        }


        mutate { 
            add_field => { "executionhost" => "%{[host][name]}" } 
        }

        # we add the logfile as a new field. But only the filename, not entire path
        mutate {
            split => ["source", "/"]
            add_field => { "logfile" => "%{[source][-1]}"  } 
        }

        prune {
            whitelist_names => ["globaljobid", 
                                "jobid", 
                                "user", 
                                "submithost", 
                                "starttime", 
                                "endtime", 
                                "status", 
                                "reason", 
                                "executionhost", 
                                "logfile", 
                                "explanation"]
        }


    # =========================================================================
    #                           startd_history 
    # -------------------------------------------------------------------------
    # here we process the events from the startd_history file
    # FileBeat marks these Events with extra field "log_type" = "history"
    # =========================================================================

    } else if [fields][log_type] == "history" {

        grok {
            pattern_definitions => { "END_OF_JOB" => "\*\*\* Offset" }
            match => { "message" => [
                '^GlobalJobId = "%{GREEDYDATA:globaljobid}"',
                '^RemoteUserCpu = %{NUMBER:remoteusercpu:float}',
                '^EnteredCurrentStatus = %{NUMBER:currentstatus_timestamp}',
                '^JobStartDate = %{NUMBER:jobstarttime}',
                '^%{END_OF_JOB:endofjob}'
            ] }
        }

        ruby { 
            code => '
                if event.get("jobstarttime")
                    $starttime = Time.at(event.get("jobstarttime").to_i).strftime("%m/%d/%y %H:%M:%S") 
                    event.cancel
                elsif event.get("globaljobid")
                    $globaljobid = event.get("globaljobid")
                    event.cancel
                elsif event.get("remoteusercpu")
                    $remoteusercpu = event.get("remoteusercpu")
                    event.cancel
                elsif event.get("currentstatus_timestamp")
                    $currentstatus_timestamp = event.get("currentstatus_timestamp")
                    event.cancel
                elsif event.get("endofjob")
                    if (Time.now.to_i - $currentstatus_timestamp.to_i) < (3600*24*10)
                        event.set("globaljobid", $globaljobid)
                        event.set("remoteusercpu", $remoteusercpu)
                        event.set("starttime", $starttime)
                    else
                        event.cancel
                    end
                else
                    event.cancel          
                end
            '
        }

        prune {
            whitelist_names => ["globaljobid", 
                                "remoteusercpu", 
                                "starttime"]
        }

    }

}


#output {
#    stdout { codec => rubydebug }
#}

output {
    lumberjack {
        codec => json
        hosts => "************************"
        ssl_certificate => "/etc/logstash/certs/lumberjack.cert"
        port => 5555
    }
}

